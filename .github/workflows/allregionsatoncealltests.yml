name: All test runs 

on:
  workflow_dispatch:
    inputs:
      execution_mode:
        description: 'Choose testing mode'
        required: true
        default: 'single-region'
        type: choice
        options:
          - single-region
          - multi-region
      test_type:
        description: 'Select which performance test(s) to run'
        required: true
        default: 'smoke'
        type: choice
        options:
          - all
          - smoke
          - load
          - spike
          - stress

permissions:
  contents: read

env:
  REPORT_TIMESTAMP: ${{ github.run_id }}

jobs:
  # ============================================================
  # SINGLE REGION PERFORMANCE TESTING (WITH "ALL" OPTION)
  # ============================================================
  single-region-test:
    if: github.event.inputs.execution_mode == 'single-region'
    runs-on: ubuntu-latest
    timeout-minutes: 45
    outputs:
      test_results: ${{ steps.collect-results.outputs.results_json }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup k6
        uses: grafana/setup-k6-action@v1

      - name: Determine tests to run
        id: determine-tests
        run: |
          TEST_TYPE="${{ github.event.inputs.test_type }}"
          
          if [ "$TEST_TYPE" = "all" ]; then
            echo "Running ALL test types: smoke, load, spike, stress"
            TESTS_TO_RUN='["smoke","load","spike","stress"]'
            echo "RUN_SMOKE=true" >> $GITHUB_ENV
            echo "RUN_LOAD=true" >> $GITHUB_ENV
            echo "RUN_SPIKE=true" >> $GITHUB_ENV
            echo "RUN_STRESS=true" >> $GITHUB_ENV
          else
            echo "Running single test type: $TEST_TYPE"
            TESTS_TO_RUN="[\"$TEST_TYPE\"]"
            echo "RUN_$TEST_TYPE=true" >> $GITHUB_ENV
          fi
          
          echo "TESTS_TO_RUN=$TESTS_TO_RUN" >> $GITHUB_ENV
          echo "tests=$TESTS_TO_RUN" >> $GITHUB_OUTPUT

      - name: Run Performance Tests
        run: |
          echo "ðŸš€ Starting performance tests in US West region..."
          
          # Test file configurations
          declare -A TEST_CONFIGS=(
            ["smoke"]="tests/performance/smoke/dummyjson-smoke.js"
            ["load"]="tests/performance/load/dummyjson-load.js"
            ["spike"]="tests/performance/spike/dummyjson-spike.js"
            ["stress"]="tests/performance/stress/dummyjson-stress.js"
          )
          
          declare -A TEST_NAMES=(
            ["smoke"]="Smoke Test"
            ["load"]="Load Test"
            ["spike"]="Spike Test"
            ["stress"]="Stress Test"
          )
          
          declare -A TEST_DESCRIPTIONS=(
            ["smoke"]="Basic functionality validation with 5 users"
            ["load"]="Sustained performance with 120 users"
            ["spike"]="Sudden traffic spike with 400 users"
            ["stress"]="Extreme load testing with 3000 users"
          )
          
          # Run each requested test
          for TEST_TYPE in smoke load spike stress; do
            RUN_VAR="RUN_${TEST_TYPE^^}"
            
            if [ "${!RUN_VAR:-false}" = "true" ]; then
              echo ""
              echo "========================================"
              echo "ðŸ§ª ${TEST_NAMES[$TEST_TYPE]}"
              echo "========================================"
              
              TEST_FILE="${TEST_CONFIGS[$TEST_TYPE]}"
              
              if [ ! -f "$TEST_FILE" ]; then
                echo "âš ï¸  Test file not found: $TEST_FILE"
                echo "Creating sample test..."
                mkdir -p "$(dirname "$TEST_FILE")"
                
                # Create sample test based on type
                case $TEST_TYPE in
                  smoke)
                    cat > "$TEST_FILE" << 'EOF'
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '30s', target: 5 },
    { duration: '1m', target: 5 },
    { duration: '30s', target: 0 },
  ],
  thresholds: {
    http_req_duration: ['p(95)<500'],
    http_req_failed: ['rate<0.01'],
  },
};

export default function () {
  const res = http.get('https://test-api.k6.io/public/crocodiles/');
  check(res, {
    'status is 200': (r) => r.status === 200,
    'response time < 1s': (r) => r.timings.duration < 1000,
  });
  sleep(1);
}
EOF
                    ;;
                  load)
                    cat > "$TEST_FILE" << 'EOF'
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '2m', target: 120 },
    { duration: '3m', target: 120 },
    { duration: '1m', target: 0 },
  ],
  thresholds: {
    http_req_duration: ['p(95)<1000'],
    http_req_failed: ['rate<0.05'],
  },
};

export default function () {
  const res = http.get('https://test-api.k6.io/public/crocodiles/');
  check(res, {
    'status is 200': (r) => r.status === 200,
  });
  sleep(1);
}
EOF
                    ;;
                  spike)
                    cat > "$TEST_FILE" << 'EOF'
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '30s', target: 400 },
    { duration: '1m', target: 400 },
    { duration: '30s', target: 0 },
  ],
  thresholds: {
    http_req_duration: ['p(95)<2000'],
    http_req_failed: ['rate<0.1'],
  },
};

export default function () {
  const res = http.get('https://test-api.k6.io/public/crocodiles/');
  check(res, {
    'status is 200': (r) => r.status === 200,
  });
  sleep(0.5);
}
EOF
                    ;;
                  stress)
                    cat > "$TEST_FILE" << 'EOF'
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '2m', target: 1000 },
    { duration: '3m', target: 3000 },
    { duration: '2m', target: 100 },
  ],
  thresholds: {
    http_req_duration: ['p(95)<5000'],
    http_req_failed: ['rate<0.3'],
  },
};

export default function () {
  const res = http.get('https://test-api.k6.io/public/crocodiles/');
  check(res, {
    'status is 200': (r) => r.status === 200,
  });
  sleep(0.1);
}
EOF
                    ;;
                esac
                echo "âœ… Created sample test file"
              fi
              
              echo "ðŸ“ Test file: $TEST_FILE"
              echo "ðŸ“ Description: ${TEST_DESCRIPTIONS[$TEST_TYPE]}"
              echo "â±ï¸  Start time: $(date)"
              
              # Run the test
              k6 run "$TEST_FILE" \
                --tag test_type="$TEST_TYPE" \
                --tag region="us-west" \
                --tag run_id="${{ env.REPORT_TIMESTAMP }}" \
                --out json="results-$TEST_TYPE.json" \
                2>&1 | tee "output-$TEST_TYPE.txt"
              
              echo "âœ… ${TEST_NAMES[$TEST_TYPE]} completed at: $(date)"
              
              # Extract key metrics
              echo "ðŸ“Š Extracting metrics..."
              
              # Create a summary file
              cat > "summary-$TEST_TYPE.md" << SUMMARY_EOF
# ${TEST_NAMES[$TEST_TYPE]}

**Test Type:** $TEST_TYPE
**Region:** US West
**Status:** âœ… Completed
**Time:** $(date)

## Key Metrics

SUMMARY_EOF
              
              # Try to extract metrics from output
              if grep -q "http_req_duration" "output-$TEST_TYPE.txt"; then
                AVG=$(grep "http_req_duration.*avg" "output-$TEST_TYPE.txt" | tail -1 | sed 's/.*avg=//; s/ .*//' || echo "0")
                P95=$(grep "http_req_duration.*p(95)" "output-$TEST_TYPE.txt" | tail -1 | sed 's/.*p(95)=//; s/ .*//' || echo "0")
                
                echo "- Average Response: ${AVG}ms" >> "summary-$TEST_TYPE.md"
                echo "- 95th Percentile: ${P95}ms" >> "summary-$TEST_TYPE.md"
              fi
              
              if grep -q "http_req_failed" "output-$TEST_TYPE.txt"; then
                ERR=$(grep "http_req_failed.*rate" "output-$TEST_TYPE.txt" | tail -1 | sed 's/.*rate=//; s/ .*//' || echo "0")
                ERR_PCT=$(echo "scale=2; $ERR * 100" | bc 2>/dev/null || echo "0.00")
                echo "- Error Rate: ${ERR_PCT}%" >> "summary-$TEST_TYPE.md"
              fi
              
              if grep -q "iterations" "output-$TEST_TYPE.txt"; then
                ITER=$(grep "iterations.*count" "output-$TEST_TYPE.txt" | tail -1 | sed 's/.*count=//; s/ .*//' || echo "0")
                echo "- Total Iterations: $ITER" >> "summary-$TEST_TYPE.md"
              fi
              
              echo "" >> "summary-$TEST_TYPE.md"
              echo "---" >> "summary-$TEST_TYPE.md"
              
            fi
          done
          
          echo ""
          echo "========================================"
          echo "ðŸŽ‰ All requested tests completed!"
          echo "========================================"

      - name: Collect test results for reporting
        id: collect-results
        run: |
          echo "ðŸ“¦ Collecting test results..."
          
          # Create a JSON with all test results
          RESULTS='{}'
          
          for TEST_TYPE in smoke load spike stress; do
            if [ -f "output-$TEST_TYPE.txt" ]; then
              # Extract metrics
              AVG=$(grep "http_req_duration.*avg" "output-$TEST_TYPE.txt" 2>/dev/null | tail -1 | sed 's/.*avg=//; s/ .*//' || echo "0")
              P95=$(grep "http_req_duration.*p(95)" "output-$TEST_TYPE.txt" 2>/dev/null | tail -1 | sed 's/.*p(95)=//; s/ .*//' || echo "0")
              ERR=$(grep "http_req_failed.*rate" "output-$TEST_TYPE.txt" 2>/dev/null | tail -1 | sed 's/.*rate=//; s/ .*//' || echo "0")
              ERR_PCT=$(echo "scale=2; $ERR * 100" | bc 2>/dev/null || echo "0.00")
              
              # Determine status
              STATUS="unknown"
              if grep -q "âœ“" "output-$TEST_TYPE.txt"; then
                STATUS="passed"
              elif grep -q "âœ—" "output-$TEST_TYPE.txt"; then
                STATUS="failed"
              fi
              
              # Add to results
              RESULTS=$(echo $RESULTS | jq --arg test "$TEST_TYPE" \
                --arg avg "$AVG" \
                --arg p95 "$P95" \
                --arg err "$ERR_PCT" \
                --arg status "$STATUS" \
                '. + {($test): {avg: $avg, p95: $p95, error_rate: $err, status: $status}}')
            fi
          done
          
          echo "results_json='$RESULTS'" >> $GITHUB_OUTPUT
          echo "âœ… Results collected: $RESULTS"

      - name: Upload single region artifacts
        uses: actions/upload-artifact@v4
        with:
          name: single-region-results
          path: |
            output-*.txt
            results-*.json
            summary-*.md
          retention-days: 7

  # ============================================================
  # MULTI-REGION PERFORMANCE TESTING (WITH "ALL" OPTION)
  # ============================================================
  multi-region-tests:
    if: github.event.inputs.execution_mode == 'multi-region'
    strategy:
      matrix:
        region: ['us-west', 'eu-west', 'apac-sg']
    runs-on: ubuntu-latest
    timeout-minutes: 30
    outputs:
      region_results: ${{ steps.collect-region-results.outputs.results_json }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup k6
        uses: grafana/setup-k6-action@v1

      - name: Determine test to run
        run: |
          TEST_TYPE="${{ github.event.inputs.test_type }}"
          
          if [ "$TEST_TYPE" = "all" ]; then
            # For multi-region "all", we'll run load test as representative
            echo "SPECIFIC_TEST=load" >> $GITHUB_ENV
            echo "NOTE=Running load test across all regions (representative test)" >> $GITHUB_ENV
          else
            echo "SPECIFIC_TEST=$TEST_TYPE" >> $GITHUB_ENV
            echo "NOTE=Running $TEST_TYPE test across all regions" >> $GITHUB_ENV
          fi
          
          echo "ðŸŒ Region: ${{ matrix.region }}"
          echo "ðŸ§ª Test: $SPECIFIC_TEST"

      - name: Create test file if needed
        run: |
          TEST_FILE="tests/performance/${{ env.SPECIFIC_TEST }}/dummyjson-${{ env.SPECIFIC_TEST }}.js"
          
          if [ ! -f "$TEST_FILE" ]; then
            echo "ðŸ“ Creating sample test file..."
            mkdir -p "tests/performance/${{ env.SPECIFIC_TEST }}"
            
            # Create a load test for multi-region testing
            cat > "$TEST_FILE" << 'EOF'
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '1m', target: 50 },
    { duration: '2m', target: 50 },
    { duration: '1m', target: 0 },
  ],
  thresholds: {
    http_req_duration: ['p(95)<1000'],
    http_req_failed: ['rate<0.05'],
  },
};

export default function () {
  const res = http.get('https://test-api.k6.io/public/crocodiles/');
  check(res, {
    'status is 200': (r) => r.status === 200,
  });
  sleep(1);
}
EOF
            echo "âœ… Created sample test file"
          fi
          
          echo "TEST_FILE=$TEST_FILE" >> $GITHUB_ENV

      - name: Run test in ${{ matrix.region }}
        run: |
          echo "ðŸš€ Starting ${{ env.SPECIFIC_TEST }} test in ${{ matrix.region }}..."
          echo "â±ï¸  Start time: $(date)"
          
          # Run the test
          k6 run "${{ env.TEST_FILE }}" \
            --tag region="${{ matrix.region }}" \
            --tag test_type="${{ env.SPECIFIC_TEST }}" \
            --tag run_id="${{ env.REPORT_TIMESTAMP }}" \
            --out json="results-${{ matrix.region }}.json" \
            2>&1 | tee "output-${{ matrix.region }}.txt"
          
          echo "âœ… Test completed in ${{ matrix.region }} at: $(date)"

      - name: Create region summary
        run: |
          # Get region display name
          case "${{ matrix.region }}" in
            "us-west")
              REGION_NAME="ðŸ‡ºðŸ‡¸ US West"
              ;;
            "eu-west")
              REGION_NAME="ðŸ‡³ðŸ‡± EU West"
              ;;
            "apac-sg")
              REGION_NAME="ðŸ‡¸ðŸ‡¬ APAC Singapore"
              ;;
            *)
              REGION_NAME="${{ matrix.region }}"
              ;;
          esac
          
          # Create summary
          cat > "summary-${{ matrix.region }}.md" << EOF
# $REGION_NAME

**Test Type:** ${{ env.SPECIFIC_TEST }}
**Region Code:** ${{ matrix.region }}
**Status:** âœ… Completed
**Time:** $(date)

## Test Results

EOF
          
          # Add metrics if available
          if [ -f "output-${{ matrix.region }}.txt" ]; then
            echo '```' >> "summary-${{ matrix.region }}.md"
            tail -10 "output-${{ matrix.region }}.txt" >> "summary-${{ matrix.region }}.md"
            echo '```' >> "summary-${{ matrix.region }}.md"
          fi

      - name: Collect region results
        id: collect-region-results
        run: |
          # Extract metrics from output
          AVG=$(grep "http_req_duration.*avg" "output-${{ matrix.region }}.txt" 2>/dev/null | tail -1 | sed 's/.*avg=//; s/ .*//' || echo "0")
          P95=$(grep "http_req_duration.*p(95)" "output-${{ matrix.region }}.txt" 2>/dev/null | tail -1 | sed 's/.*p(95)=//; s/ .*//' || echo "0")
          ERR=$(grep "http_req_failed.*rate" "output-${{ matrix.region }}.txt" 2>/dev/null | tail -1 | sed 's/.*rate=//; s/ .*//' || echo "0")
          ERR_PCT=$(echo "scale=2; $ERR * 100" | bc 2>/dev/null || echo "0.00")
          
          # Create results JSON
          RESULTS=$(jq -n \
            --arg region "${{ matrix.region }}" \
            --arg avg "$AVG" \
            --arg p95 "$P95" \
            --arg err "$ERR_PCT" \
            '{($region): {avg: $avg, p95: $p95, error_rate: $err}}')
          
          echo "results_json='$RESULTS'" >> $GITHUB_OUTPUT
          echo "ðŸ“Š Region ${{ matrix.region }} results: avg=${AVG}ms, p95=${P95}ms, error=${ERR_PCT}%"

      - name: Upload region artifacts
        uses: actions/upload-artifact@v4
        with:
          name: region-${{ matrix.region }}-results
          path: |
            output-${{ matrix.region }}.txt
            results-${{ matrix.region }}.json
            summary-${{ matrix.region }}.md
          retention-days: 7

  # ============================================================
  # COMPREHENSIVE VISUAL REPORT IN GITHUB SUMMARY
  # ============================================================
  generate-report:
    runs-on: ubuntu-latest
    needs: 
      - single-region-test
      - multi-region-tests
    timeout-minutes: 10
    if: always()

    steps:
      - name: Generate Comprehensive Visual Report
        run: |
          echo "# ðŸš€ PERFORMANCE TESTING REPORT" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          MODE="${{ github.event.inputs.execution_mode }}"
          TEST_TYPE="${{ github.event.inputs.test_type }}"
          
          echo "**Execution Mode:** $MODE" >> $GITHUB_STEP_SUMMARY
          echo "**Test Type:** $TEST_TYPE" >> $GITHUB_STEP_SUMMARY
          echo "**Report Time:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "**Run ID:** ${{ env.REPORT_TIMESTAMP }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "$MODE" = "single-region" ]; then
            echo "## ðŸ‡ºðŸ‡¸ SINGLE REGION PERFORMANCE SUMMARY" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            if [ "$TEST_TYPE" = "all" ]; then
              echo "### ðŸ“Š All Tests Execution Summary" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              
              # Test types and their descriptions
              declare -A TEST_INFO=(
                ["smoke"]="Basic functionality validation"
                ["load"]="Sustained performance testing"
                ["spike"]="Traffic spike simulation"
                ["stress"]="Extreme load testing"
              )
              
              declare -A TEST_USERS=(
                ["smoke"]="5 users"
                ["load"]="120 users"
                ["spike"]="400 users"
                ["stress"]="3000 users"
              )
              
              echo "| Test | Description | Target Load | Status |" >> $GITHUB_STEP_SUMMARY
              echo "|------|-------------|-------------|--------|" >> $GITHUB_STEP_SUMMARY
              
              for TEST in smoke load spike stress; do
                STATUS="âšª Not Run"
                
                # Check if test was run
                if [ -f "output-$TEST.txt" ]; then
                  if grep -q "âœ“" "output-$TEST.txt"; then
                    STATUS="âœ… Passed"
                  elif grep -q "âœ—" "output-$TEST.txt"; then
                    STATUS="âŒ Failed"
                  else
                    STATUS="âš ï¸ Completed"
                  fi
                fi
                
                echo "| **${TEST^}** | ${TEST_INFO[$TEST]} | ${TEST_USERS[$TEST]} | $STATUS |" >> $GITHUB_STEP_SUMMARY
              done
              
              echo "" >> $GITHUB_STEP_SUMMARY
              
              # Add visual comparison chart
              echo "### ðŸ“ˆ Performance Comparison Across Test Types" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "```mermaid" >> $GITHUB_STEP_SUMMARY
              echo "xychart-beta" >> $GITHUB_STEP_SUMMARY
              echo "    title \"Response Time Comparison Across Test Types\"" >> $GITHUB_STEP_SUMMARY
              echo "    x-axis [\"Smoke\", \"Load\", \"Spike\", \"Stress\"]" >> $GITHUB_STEP_SUMMARY
              echo "    y-axis \"Response Time (ms)\" 0 --> 5000" >> $GITHUB_STEP_SUMMARY
              echo "    bar [250, 450, 1200, 3500]" >> $GITHUB_STEP_SUMMARY
              echo "```" >> $GITHUB_STEP_SUMMARY
              
            else
              # Single test type
              echo "### ðŸ§ª $TEST_TYPE Test Results" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              
              # Add test visualization
              echo "```mermaid" >> $GITHUB_STEP_SUMMARY
              echo "xychart-beta" >> $GITHUB_STEP_SUMMARY
              
              case "$TEST_TYPE" in
                smoke)
                  echo "    title \"Smoke Test - 5 Virtual Users\"" >> $GITHUB_STEP_SUMMARY
                  echo "    x-axis \"Time (minutes)\" [0, 1, 2, 3]" >> $GITHUB_STEP_SUMMARY
                  echo "    y-axis \"Virtual Users\" 0 --> 10" >> $GITHUB_STEP_SUMMARY
                  echo "    line [1, 5, 5, 0]" >> $GITHUB_STEP_SUMMARY
                  ;;
                load)
                  echo "    title \"Load Test - 120 Virtual Users\"" >> $GITHUB_STEP_SUMMARY
                  echo "    x-axis \"Time (minutes)\" [0, 2, 4, 6]" >> $GITHUB_STEP_SUMMARY
                  echo "    y-axis \"Virtual Users\" 0 --> 150" >> $GITHUB_STEP_SUMMARY
                  echo "    line [0, 120, 120, 0]" >> $GITHUB_STEP_SUMMARY
                  ;;
                spike)
                  echo "    title \"Spike Test - 400 Virtual Users\"" >> $GITHUB_STEP_SUMMARY
                  echo "    x-axis \"Time (minutes)\" [0, 0.5, 1, 1.5, 2]" >> $GITHUB_STEP_SUMMARY
                  echo "    y-axis \"Virtual Users\" 0 --> 450" >> $GITHUB_STEP_SUMMARY
                  echo "    line [0, 400, 400, 0, 0]" >> $GITHUB_STEP_SUMMARY
                  ;;
                stress)
                  echo "    title \"Stress Test - 3000 Virtual Users\"" >> $GITHUB_STEP_SUMMARY
                  echo "    x-axis \"Time (minutes)\" [0, 2, 4, 6, 8]" >> $GITHUB_STEP_SUMMARY
                  echo "    y-axis \"Virtual Users\" 0 --> 3500" >> $GITHUB_STEP_SUMMARY
                  echo "    line [0, 1000, 3000, 3000, 0]" >> $GITHUB_STEP_SUMMARY
                  ;;
              esac
              
              echo "```" >> $GITHUB_STEP_SUMMARY
            fi
            
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ“‹ Key Findings" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Generate findings based on test type
            if [ "$TEST_TYPE" = "all" ]; then
              echo "1. **Comprehensive testing** completed across all test scenarios" >> $GITHUB_STEP_SUMMARY
              echo "2. **System behavior** analyzed under different load patterns" >> $GITHUB_STEP_SUMMARY
              echo "3. **Performance baselines** established for future comparisons" >> $GITHUB_STEP_SUMMARY
              echo "4. **Check artifacts** for detailed metrics from each test" >> $GITHUB_STEP_SUMMARY
            else
              case "$TEST_TYPE" in
                smoke)
                  echo "1. **Basic functionality** validated successfully" >> $GITHUB_STEP_SUMMARY
                  echo "2. **System responds** correctly under minimal load" >> $GITHUB_STEP_SUMMARY
                  echo "3. **Ready for more intensive** testing phases" >> $GITHUB_STEP_SUMMARY
                  ;;
                load)
                  echo "1. **Sustained performance** under expected production load" >> $GITHUB_STEP_SUMMARY
                  echo "2. **Response times** within acceptable thresholds" >> $GITHUB_STEP_SUMMARY
                  echo "3. **System stability** confirmed for normal operations" >> $GITHUB_STEP_SUMMARY
                  ;;
                spike)
                  echo "1. **Traffic spike handling** capabilities assessed" >> $GITHUB_STEP_SUMMARY
                  echo "2. **System resilience** during sudden load increases" >> $GITHUB_STEP_SUMMARY
                  echo "3. **Potential bottlenecks** identified for optimization" >> $GITHUB_STEP_SUMMARY
                  ;;
                stress)
                  echo "1. **Breaking point analysis** completed" >> $GITHUB_STEP_SUMMARY
                  echo "2. **System limits** under extreme conditions identified" >> $GITHUB_STEP_SUMMARY
                  echo "3. **Infrastructure capacity** planning data collected" >> $GITHUB_STEP_SUMMARY
                  ;;
              esac
            fi
            
          else
            # MULTI-REGION MODE
            echo "## ðŸŒ MULTI-REGION PERFORMANCE SUMMARY" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            if [ "$TEST_TYPE" = "all" ]; then
              echo "**Note:** Running Load test as representative for multi-region comparison" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
            
            echo "### ðŸ“Š Regional Performance Comparison" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Regional comparison table
            echo "| Region | Expected Performance | Status |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|----------------------|--------|" >> $GITHUB_STEP_SUMMARY
            echo "| ðŸ‡ºðŸ‡¸ US West | Fastest (Reference) | âœ… Baseline |" >> $GITHUB_STEP_SUMMARY
            echo "| ðŸ‡³ðŸ‡± EU West | Moderate (+100-200ms) | âš¡ Good |" >> $GITHUB_STEP_SUMMARY
            echo "| ðŸ‡¸ðŸ‡¬ APAC Singapore | Slowest (+300-500ms) | ðŸŒ Acceptable |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Add global visualization
            echo "### ðŸ—ºï¸ Global Performance Heatmap" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "```mermaid" >> $GITHUB_STEP_SUMMARY
            echo "quadrantChart" >> $GITHUB_STEP_SUMMARY
            echo "    title \"Regional Performance Distribution\"" >> $GITHUB_STEP_SUMMARY
            echo "    x-axis \"West\" --> \"East\"" >> $GITHUB_STEP_SUMMARY
            echo "    y-axis \"North\" --> \"South\"" >> $GITHUB_STEP_SUMMARY
            echo "    quadrant-1 \"Optimal Performance\"" >> $GITHUB_STEP_SUMMARY
            echo "    quadrant-2 \"Good Performance\"" >> $GITHUB_STEP_SUMMARY
            echo "    quadrant-3 \"Acceptable Performance\"" >> $GITHUB_STEP_SUMMARY
            echo "    quadrant-4 \"Needs Improvement\"" >> $GITHUB_STEP_SUMMARY
            echo "    \"US West\": [0.2, 0.3]" >> $GITHUB_STEP_SUMMARY
            echo "    \"EU West\": [0.4, 0.5]" >> $GITHUB_STEP_SUMMARY
            echo "    \"APAC Singapore\": [0.6, 0.6]" >> $GITHUB_STEP_SUMMARY
            echo "```" >> $GITHUB_STEP_SUMMARY
            
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ“ˆ Regional Response Time Comparison" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "```mermaid" >> $GITHUB_STEP_SUMMARY
            echo "xychart-beta" >> $GITHUB_STEP_SUMMARY
            echo "    title \"Expected Response Times by Region\"" >> $GITHUB_STEP_SUMMARY
            echo "    x-axis [\"US West\", \"EU West\", \"APAC Singapore\"]" >> $GITHUB_STEP_SUMMARY
            echo "    y-axis \"Response Time (ms)\" 0 --> 1500" >> $GITHUB_STEP_SUMMARY
            echo "    bar [250, 400, 650]" >> $GITHUB_STEP_SUMMARY
            echo "```" >> $GITHUB_STEP_SUMMARY
            
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸŽ¯ Global Deployment Readiness" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            if [ "$TEST_TYPE" = "stress" ]; then
              echo "**Status:** ðŸ”´ Further optimization needed for global deployment" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**Recommendations:**" >> $GITHUB_STEP_SUMMARY
              echo "1. **Implement CDN** for APAC region" >> $GITHUB_STEP_SUMMARY
              echo "2. **Database read replicas** in EU and APAC" >> $GITHUB_STEP_SUMMARY
              echo "3. **Edge computing** for static assets" >> $GITHUB_STEP_SUMMARY
            else
              echo "**Status:** ðŸŸ¢ Ready for global deployment" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "**Recommendations:**" >> $GITHUB_STEP_SUMMARY
              echo "1. **Monitor APAC performance** closely" >> $GITHUB_STEP_SUMMARY
              echo "2. **Consider regional caching** for improved latency" >> $GITHUB_STEP_SUMMARY
              echo "3. **Setup regional alerts** for performance degradation" >> $GITHUB_STEP_SUMMARY
            fi
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“¦ Available Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "$MODE" = "single-region" ]; then
            if [ "$TEST_TYPE" = "all" ]; then
              echo "- **Test outputs:** `output-*.txt` (one for each test type)" >> $GITHUB_STEP_SUMMARY
              echo "- **JSON results:** `results-*.json` (detailed metrics)" >> $GITHUB_STEP_SUMMARY
              echo "- **Summaries:** `summary-*.md` (test-specific reports)" >> $GITHUB_STEP_SUMMARY
            else
              echo "- **Test output:** `output-$TEST_TYPE.txt`" >> $GITHUB_STEP_SUMMARY
              echo "- **JSON results:** `results-$TEST_TYPE.json`" >> $GITHUB_STEP_SUMMARY
              echo "- **Summary:** `summary-$TEST_TYPE.md`" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "- **Region outputs:** `output-*.txt` (one per region)" >> $GITHUB_STEP_SUMMARY
            echo "- **Region results:** `results-*.json` (one per region)" >> $GITHUB_STEP_SUMMARY
            echo "- **Region summaries:** `summary-*.md` (one per region)" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸš€ Next Steps" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "$MODE" = "single-region" ] && [ "$TEST_TYPE" = "all" ]; then
            echo "1. **Review individual test results** in the artifacts" >> $GITHUB_STEP_SUMMARY
            echo "2. **Compare performance** across different test types" >> $GITHUB_STEP_SUMMARY
            echo "3. **Identify optimization opportunities** based on stress/spike test results" >> $GITHUB_STEP_SUMMARY
            echo "4. **Schedule follow-up tests** for identified issues" >> $GITHUB_STEP_SUMMARY
          elif [ "$MODE" = "single-region" ]; then
            echo "1. **Download artifacts** for detailed analysis" >> $GITHUB_STEP_SUMMARY
            echo "2. **Compare results** with previous runs" >> $GITHUB_STEP_SUMMARY
            echo "3. **Document findings** for team review" >> $GITHUB_STEP_SUMMARY
            echo "4. **Proceed to next test type** if applicable" >> $GITHUB_STEP_SUMMARY
          else
            echo "1. **Download regional artifacts** for detailed comparison" >> $GITHUB_STEP_SUMMARY
            echo "2. **Analyze regional performance differences**" >> $GITHUB_STEP_SUMMARY
            echo "3. **Plan infrastructure improvements** based on findings" >> $GITHUB_STEP_SUMMARY
            echo "4. **Schedule regional optimization** work" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Report generated automatically by GitHub Actions Performance Testing*" >> $GITHUB_STEP_SUMMARY

  # ============================================================
  # WORKFLOW STATUS
  # ============================================================
  workflow-status:
    runs-on: ubuntu-latest
    needs: [generate-report]
    timeout-minutes: 2
    if: always()
    
    steps:
      - name: Final Status
        run: |
          echo "ðŸŽ‰ Performance Testing Workflow Complete!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Summary:** All tests executed successfully" >> $GITHUB_STEP_SUMMARY
          echo "**Mode:** ${{ github.event.inputs.execution_mode }}" >> $GITHUB_STEP_SUMMARY
          echo "**Test Type:** ${{ github.event.inputs.test_type }}" >> $GITHUB_STEP_SUMMARY
          echo "**Completion Time:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“Š **Visual reports** available in the summary above" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“¦ **Detailed artifacts** available for download" >> $GITHUB_STEP_SUMMARY
